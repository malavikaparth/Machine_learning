{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbRTG-QL17Ez",
        "outputId": "b1523dc0-be42-495d-8926-2c56c8fc9315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-09 16:20:21--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  19.8MB/s    in 16s     \n",
            "\n",
            "2023-01-09 16:20:37 (14.7 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the images are there\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename='/content/tiny-imagenet-200/train/n01443537/images/n01443537_0.JPEG'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "W-sVUPJc2Giy",
        "outputId": "4640ad95-dc70-49db-fd7d-de92826f99ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import os, time, shutil, argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "ZayVn0Ux2KFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A folder for loading image and converting them to LAB color space\n",
        "class LABImageFolder(datasets.ImageFolder):\n",
        "  '''Custom images folder, which converts images to grayscale before loading'''\n",
        "  def __getitem__(self, index):\n",
        "    # As i dont have target removing that path, target = self.imgs[index]\n",
        "    path,_ = self.imgs[index]\n",
        "    #This code loads each image in the folder provided by the path using self loader method.\n",
        "    img = self.loader(path)\n",
        "    # As i dont have anything to transform removing that if self.transform is not None:\n",
        "      # This too img_original = self.transform(img)\n",
        "    # This too img_original = np.asarray(img_original)\n",
        "\n",
        "    #This line converts the image to the LAB color space using the rgb2lab function\n",
        "    img_lab = rgb2lab(img)\n",
        "\n",
        "    #This line scales the values in the LAB image to be between 0 and 1.\n",
        "    img_l = img_lab[:,:,0:1]\n",
        "    #This line extracts the \"ab\" channels from the LAB image.\n",
        "    img_ab = img_lab[:, :, 1:3]\n",
        "    #This line converts the \"l\" channel to a PyTorch tensor.\n",
        "    tens_l = torch.from_numpy(img_l.transpose((2, 0, 1))).float()\n",
        "    #This line converts the \"ab\" channels to a PyTorch tensor.\n",
        "    tens_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "    #Not needed img_original = rgb2gray(img_original)\n",
        "    #Not needed img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
        "    return tens_l,tens_ab"
      ],
      "metadata": {
        "id": "BSj4NNxi2OHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_imagefolder = LABImageFolder('/content/tiny-imagenet-200/train')\n",
        "train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "ARgc_nIM2RLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first image from the dataset\n",
        "tens_l, tens_ab = train_imagefolder[0]\n",
        "hist, edges = np.histogram(tens_ab, bins=313)\n",
        "#print(hist)\n",
        "#print(edges)\n"
      ],
      "metadata": {
        "id": "PbroVc_R2Twt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_val = min(tens_ab)\n",
        "max_val = max(tens_ab)\n",
        "range = max_val - min_val\n",
        "bin_size = range / 10\n",
        "bin_boundaries = []\n",
        "current_boundary = min_val\n",
        "while current_boundary <= max_val:\n",
        "  bin_boundaries.append(current_boundary)\n",
        "  current_boundary += bin_size\n",
        "in_gamut_values = []\n",
        "for value in tens_ab:\n",
        "  if value >= min_val and value <= max_val:\n",
        "    in_gamut_values.append(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "XnyCsFZEbUwD",
        "outputId": "88b95239-fd04-4a6c-b315-45df297c13d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6149744653c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbin_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_imagefolder = LABImageFolder('/content/tiny-imagenet-200/val')\n",
        "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "OiL8cxrJ-JMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first image from the dataset\n",
        "tens_l, tens_ab = val_imagefolder[0]\n",
        "print(tens_ab)\n",
        "print(tens_ab.size())\n",
        "print(tens_l.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gtxpMwb-XeY",
        "outputId": "80b342fa-dc67-40dd-817a-1ef46fd0d486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ -3.5201,   2.7465,  11.3694,  ...,  -4.5921,  -5.5708,  -5.9544],\n",
            "         [ -9.3024,  -1.7027,   9.5663,  ...,  -4.7139,  -4.7824,  -5.6851],\n",
            "         [-13.4133,  -6.8106,   2.6426,  ...,  -4.3594,  -5.9405,  -6.2038],\n",
            "         ...,\n",
            "         [ -0.4560,  -0.4523,  -0.4413,  ...,  -0.3873,  -0.3856,  -0.3876],\n",
            "         [ -0.4580,  -0.4651,  -0.4662,  ...,  -0.3866,  -0.3843,  -0.3853],\n",
            "         [ -0.3943,  -0.3974,  -0.3966,  ...,  -0.3890,  -0.3850,  -0.3853]],\n",
            "\n",
            "        [[ -4.0786,  -4.1801,  -4.9396,  ...,   8.3960,  10.0222,  11.1804],\n",
            "         [ -1.2838,  -2.3903,  -4.1715,  ...,   8.6583,   8.8096,  10.2742],\n",
            "         [ -0.6111,  -1.7768,  -3.5280,  ...,   7.5859,   8.1285,   9.0793],\n",
            "         ...,\n",
            "         [  1.2609,   1.2499,   1.2170,  ...,   2.1698,   2.1587,   2.1721],\n",
            "         [  2.7872,   2.9164,   2.9441,  ...,   2.1653,   2.1500,   2.1565],\n",
            "         [  2.2183,   2.2396,   2.2342,  ...,   2.1814,   2.1543,   2.1565]]])\n",
            "torch.Size([2, 64, 64])\n",
            "torch.Size([1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Tensors are usually put into bins to make it easier and more efficient to store and retrieve them.\n",
        "Bins can help you organize your tensors and make it faster to access specific ones.\n",
        "\n",
        "Quantizing an absolute value refers to the process of limiting or restricting the possible values that a quantity can take on.\n",
        "\n",
        "To quantize the ab output space into bins with grid size 10, you can first create a grid of size 10 in the ab space.\n",
        "This can be done by dividing the range of possible values for a and b by 10 to get the size of each bin, and then creating a grid of bins with this size.\n",
        "\n",
        "You can then create a grid of bins by starting at the minimum value for a and b and creating bins of size 13 until you reach the maximum value.\n",
        "\n",
        " The term \"in-gamut\" usually refers to values that fall within a certain range or set of limits.\n",
        "\n",
        "To keep the Q = 313 values which are in-gamut, you can then go through each of the 313 values and check which bin it falls into.\n",
        "If the value is within the range of values for one of the bins, you can add it to that bin.\n",
        "If it is not within the range of any of the bins, you can discard it or put it into a separate bin for out-of-gamut values.\n",
        "\n",
        "Once you have all the values organized into bins, you can treat the problem as a multinomial classification problem\n",
        "by assigning a class label to each bin and training a classifier to predict the class label given an input value.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "BwtVfwsdrGOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the tensor to a 1D array so that you can iterate through its values.\n",
        "output_space = tens_ab.flatten()\n",
        "\n",
        "# Determine the minimum and maximum values of the output space\n",
        "min_val = output_space.min()\n",
        "max_val = output_space.max()\n",
        "\n",
        "# Calculate the range of the output space\n",
        "range = max_val - min_val\n",
        "\n",
        "# Divide the range by the grid size to determine the size of each bin\n",
        "bin_size = range / 10\n",
        "\n",
        "# Create an array of bin boundaries by starting at the minimum value and adding the bin size until you reach the maximum value\n",
        "bin_boundaries = []\n",
        "current_boundary = min_val\n",
        "while current_boundary <= max_val:\n",
        "  bin_boundaries.append(current_boundary)\n",
        "  current_boundary += bin_size\n",
        "\n",
        "# Iterate through the values in the output space and assign each value to the appropriate bin by finding the bin boundary that it falls within.\n",
        "bins = []\n",
        "for value in output_space:\n",
        "  for i, boundary in enumerate(bin_boundaries):\n",
        "    if value < boundary:\n",
        "      bins.append(i)\n",
        "      break\n",
        "\n",
        "# Keep only the values that are in-gamut by discarding any values that fall outside of the desired range.\n",
        "in_gamut_values = []\n",
        "for i, value in enumerate(output_space):\n",
        "  if bins[i] == 313:\n",
        "    in_gamut_values.append(value)\n",
        "    print(in_gamut_values)"
      ],
      "metadata": {
        "id": "X2facOomjjLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}